import requests
from utils.contants import OLLAMA_MODEL
from utils.contants import OLLAMA_MODEL_ENDPOINT

def call_ollama(
    prompt: str,
    model: str = OLLAMA_MODEL,
    stream: bool = False
) -> str:
    """
    Sends a prompt to the Ollama service and returns the response.

    Parameters:
        prompt (str): The prompt to be sent.
        model (str): The model to be used (default: 'tinyllama').
        stream (bool): If True, enables streaming.

    Returns:
        str: Response generated by the model.
    """
    url = OLLAMA_MODEL_ENDPOINT
    payload = {
        'model': model,
        'prompt': prompt,
        'stream': stream
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        data = response.json()
        return data.get('response', '').strip()
    except requests.RequestException as e:
        print(f"Erro ao comunicar com a API Ollama: {e}")
        return ""