import hashlib
from diskcache import Cache
import requests
from utils.contants import OLLAMA_MODEL
from utils.contants import OLLAMA_MODEL_ENDPOINT

# Initialize the cache for Ollama responses
cache = Cache('./ollama_cache')

def _hash_prompt(prompt: str) -> str:
    """
    Generates a unique hash for the given prompt to use as a cache key.
    This helps in avoiding duplicate requests for the same prompt.
    Parameters:
        prompt (str): The prompt to be hashed.
    Returns:
        str: A SHA-256 hash of the prompt.
    """
    return hashlib.sha256(prompt.encode()).hexdigest()

def call_ollama(
    prompt: str,
    model: str = OLLAMA_MODEL,
    stream: bool = False,
    use_cache: bool = True
) -> str:
    """
    Sends a prompt to the Ollama service and returns the response.
    If the response is not cached, it will make a request to the Ollama API.
    If the request fails, it will return an empty string.
    If the response is cached, it will return the cached response.

    Parameters:
        prompt (str): The prompt to be sent.
        model (str): The model to be used (default: 'tinyllama').
        stream (bool): If True, enables streaming.
        use_cache (bool): If True, uses the cache to store and retrieve responses.

    Returns:
        str: Response generated by the model.
    """

    key = _hash_prompt(prompt)

    if use_cache and key in cache:
        return cache[key]
    
    url = OLLAMA_MODEL_ENDPOINT
    payload = {
        'model': model,
        'prompt': prompt,
        'stream': stream
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        data = response.json()
        result = data.get('response', '').strip()

        # Cache the result if caching is enabled
        if use_cache:
            cache[key] = result

        return result
    except requests.RequestException as e:
        print(f"Error communicating with the Ollama API: {e}")
        return ""