{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import roboflow\n",
    "\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Apple Silicon, check if MPS is available\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if CUDA is available and print the number of GPUs\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino de um modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset from roboflow, after labeling the images and creating the dataset\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "# create a file with the roboflow api key, or simply replace below\n",
    "api_key = os.getenv(\"ROBOFLOW_PRIVATE_API_KEY\")  # this is the PrivateAPIKey from the roboflow site settings\n",
    "\n",
    "rf = roboflow.Roboflow(api_key)\n",
    "\n",
    "# replace with your workspace and project name (you can find this in the project url on roboflow)\n",
    "project = rf.workspace(\"public-vtwkd\").project(\"pokemmo-tx0hm\")  # must be all lowercase apparently\n",
    "\n",
    "# if dataset version > 1, replace with the corresponding version\n",
    "dataset = project.version(2).download(model_format=\"yolov8\", location=\"dataset\")  # location is the path where the dataset will be saved\n",
    "# WARN: you need to check the paths in the data.yaml file after it is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# list of pre-trained models available at https://docs.ultralytics.com/models/yolov8/#performance-metrics\n",
    "model = YOLO(\"yolov8s.pt\")  # load the pre-trained model you downloaded\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='dataset/data.yaml', epochs=20, imgsz=640, device=\"cpu\") # intel/windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best version of the fine-tuned model\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new images\n",
    "confidence_level = 0.1\n",
    "input_path = 'captured_images'\n",
    "output_path = 'detections'\n",
    "class_names = model.names\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    if file.lower().endswith((\".png\")) or file.lower().endswith((\".jpg\")) or file.lower().endswith((\".jpeg\")):\n",
    "        image = cv2.imread(os.path.join(input_path, file))\n",
    "        results = model.predict(source=image, conf=confidence_level)  # generate predictions above a certain confidence, and save images\n",
    "\n",
    "        output_filename = f\"prediction_{file}\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        for result in results:\n",
    "            result.save(filename=output_filepath)\n",
    "            print(\"==== Prediction Results ====\")\n",
    "            print(\"Image: \" + os.path.join(input_path, file))\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()  # Bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "            scores = result.boxes.conf.cpu().numpy()  # Confidence score\n",
    "            labels = result.boxes.cls.cpu().numpy()  # Class index\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                class_id = labels[i]\n",
    "                class_label = class_names[class_id] if class_id in class_names else \"Unknown\"\n",
    "\n",
    "                print(f\"--- Object {i+1} ---\")\n",
    "                print(f\"Class: {class_label} (ID: {class_id})\")\n",
    "                print(f\"Bounding Box Coordinates: {boxes[i]}\")\n",
    "                print(f\"Confidence: {scores[i]:.4f}\")\n",
    "                print(\"-------------------\")\n",
    "\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência em tempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mss\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# xhost + to allow access to the display on Linux\n",
    "os.environ['DISPLAY'] = ':0'  # to avoid display error on Linux\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Function to capture the screen and return the image\n",
    "def capture_screen():\n",
    "    with mss.mss() as sct:\n",
    "        screenshot = sct.grab(sct.monitors[1])  # Capture from the main monitor\n",
    "        img = np.array(screenshot)  # Convert to image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR) # Convert BGRA to BGR\n",
    "        img_height, img_width, _ = img.shape\n",
    "\n",
    "        # Optionally, save the captured image\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S-%f\")  # Include microseconds\n",
    "        img_path = os.path.join('captured_images/pyautogui', f\"capture_{timestamp}.jpg\")\n",
    "        cv2.imwrite(img_path, img)\n",
    "\n",
    "        return img, timestamp, img_width, img_height\n",
    "\n",
    "while True:\n",
    "    img, timestamp, img_width, img_height = capture_screen()\n",
    " \n",
    "    #results = model.predict(source=img, save=True, save_txt=True, conf=0.1) \n",
    "    results = model(img)\n",
    "\n",
    "    # Extract detections (bounding boxes)\n",
    "    detections = results[0].boxes.xyxy  # Bounding boxes (x1, y1, x2, y2)\n",
    "\n",
    "    if len(detections) > 0:\n",
    "        print(f\"Detected {len(detections)} objects.\")\n",
    "\n",
    "        # Optionally, save image with detections\n",
    "        annotated_frame = results[0].plot()  # Draw bounding boxes on the image\n",
    "        result_path = os.path.join('detections', f\"result_{timestamp}.jpg\")\n",
    "        cv2.imwrite(result_path, annotated_frame)\n",
    "\n",
    "        for i, (x1, y1, x2, y2) in enumerate(detections.tolist()):\n",
    "            # Calculate the center of the object\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "\n",
    "            # Convert coordinates\n",
    "            scaled_x = int((center_x / img_width) * screen_width)\n",
    "            scaled_y = int((center_y / img_height) * screen_height)\n",
    "\n",
    "            # Move the mouse\n",
    "            pyautogui.moveTo(scaled_x, scaled_y, duration=0.3)\n",
    "            pyautogui.click()\n",
    "            print(f\"Moved to object {i+1} at ({scaled_x}, {scaled_y})\")\n",
    "\n",
    "            # Brief pause between objects\n",
    "            time.sleep(0.25)  # Adjust delay as needed\n",
    "\n",
    "    time.sleep(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
